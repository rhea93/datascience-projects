{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eagles CLV Refresh Analysis V1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# import pickle as pickle\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "# import datetime\n",
    "import json\n",
    "import logging\n",
    "from io import StringIO\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_col_names(df):\n",
    "    '''\n",
    "    takes a df and converts column names to lowercase underscore seperated\n",
    "    '''\n",
    "    df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This data is saved out on the shared drive\n",
    "# TODO: Add your input csv here\n",
    "dat = pd.read_csv(r'C:\\Users\\rhead\\Documents\\local_ds\\Model_refreshes\\eagles\\EAGLES_STAGE_CLVMODELOUTPUTFULL_7242024.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create df object that we will use for the rest of the script - preserve original dfclv\n",
    "dfclv = fix_col_names(dat.copy())\n",
    "\n",
    "# create date col for date logic\n",
    "dfclv['date'] = pd.to_datetime(dfclv['modelrundate'], utc=True).dt.date\n",
    "dfclv.sort_values(['audienceid','date'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retention Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_retention_merged_df(dfclv):\n",
    "    \"\"\"\n",
    "    Creates a merged DataFrame for Retention charts dividing the df into predictions and actuals and merging them\n",
    "    after making some adjustments.\n",
    "\n",
    "    This function processes the input DataFrame to identify customers \n",
    "    belonging to the 'STM' segment and checks if they have renewed their \n",
    "    status in the latest date. It adjusts deciles for the initial date \n",
    "    and merges initial and comparison DataFrames to produce the final \n",
    "    merged DataFrame with renewal information.\n",
    "\n",
    "    Returns:\n",
    "    Since to construct the retention chart we only require a few features from the main df\n",
    "    the returning df will also retain a subset of columns\n",
    "    pd.DataFrame: A DataFrame containing 'audienceid', 'probstm_x', 'renewed', \n",
    "                  'deciles_x', and 'date_x' with renewal status.\n",
    "    \"\"\"\n",
    "    #Selet the required columns\n",
    "    df = dfclv[['date', 'audienceid', 'clvcustomersegment', 'probstm', 'deciles']].copy()\n",
    "    \n",
    "    # select all model rundates except the actuals one\n",
    "    df_initial = df[df.date != df.date.max()]\n",
    "    \n",
    "    # make sure we only have STMs since we are analyzing the STM retention model\n",
    "    df_initial_stm = df_initial[df_initial.clvcustomersegment == 'STM']\n",
    "    \n",
    "    # this is an ad hoc fix because this model run date for Eagles had deciles from (0-9) instead of (1-10) like the rest of them\n",
    "    df_initial_stm['deciles'] = np.where(df_initial_stm.date == df_initial_stm.date.min(), df_initial_stm.deciles + 1, df_initial_stm['deciles'])\n",
    "\n",
    "    #create the actuals df\n",
    "    df_comp = df[df.date == df.date.max()]\n",
    "\n",
    "    #filter only the audienceids that are present in the predictions df\n",
    "    df_comp = df_comp[df_comp.audienceid.isin(df_initial_stm.audienceid.values.tolist())]\n",
    "\n",
    "    # create a renewed column for those customers who renewed their STM package\n",
    "    df_comp['renewed'] = np.where(df_comp.clvcustomersegment == 'STM', 1, 0)\n",
    "\n",
    "    #Merge both dataframes on audienceid\n",
    "    df_merge = df_initial_stm.merge(df_comp, how='left', on='audienceid')[['audienceid', 'probstm_x', 'renewed', 'deciles_x', 'date_x']]\n",
    "\n",
    "    #renewed will have 0 for the ones who churned and are not a part of the actuals dataframe\n",
    "    df_merge['renewed'] = np.where(df_merge.renewed.isna(), 0, df_merge.renewed)\n",
    "    return df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = create_retention_merged_df(dfclv)\n",
    "df_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_final_retention_decile_graph(df_merge, get_data=False):\n",
    "    \"\"\"\n",
    "    Displays retention graph for final slide deck along with additional labels for the mean\n",
    "    probs and actuals for each model run and all model run plus actual results together\n",
    "    \n",
    "    input:\n",
    "        df_merge: input dataframe\n",
    "        get_data: True/False to have the data output to an object\n",
    "    output:\n",
    "        score_df:  optinal output of data\n",
    "    \"\"\"\n",
    "    df_all = pd.DataFrame() #for the overall comp chart\n",
    "    \n",
    "    dates = df_merge.date_x.value_counts().index.sort_values()\n",
    "    \n",
    "    for i in dates:\n",
    "        print(i)\n",
    "        fig, ax = plt.subplots(figsize = (12, 6))\n",
    "        \n",
    "        # Create subsets grouping by the different deciles\n",
    "        deciles = df_merge[df_merge.date_x == i].groupby('deciles_x')['probstm_x'].mean()\n",
    "        score_df = deciles.to_frame()\n",
    "        score_df['deciles_x'] = score_df.index\n",
    "        score_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "        rfactual = df_merge[df_merge.date_x == i].groupby('deciles_x')['renewed'].mean()\n",
    "        rfactual.reset_index(drop=True, inplace=True)\n",
    "        score_df = pd.concat((score_df, rfactual), axis = 1)\n",
    "        score_df.rename(columns={'probstm_x':\"Model Prediction\", 'renewed':'Actual Results'}, inplace=True)\n",
    "       \n",
    "        #Reformat df shape to use in plot\n",
    "        df_forplot = pd.melt(score_df, id_vars=['deciles_x'], value_vars=['Model Prediction', 'Actual Results'], var_name='Model', value_name = 'Probability')\n",
    "        \n",
    "        ### Create Plot\n",
    "        ax= sns.lineplot(x=\"deciles_x\", y=\"Probability\", hue=\"Model\",\n",
    "                          palette= ['#1f497d', '#4290dc'], marker=\"D\", data=df_forplot) #'#b3d3f1'\n",
    "        plt.axhline(y=score_df['Actual Results'].mean(), color='#4290dc',ls='--')\n",
    "        plt.axhline(y=score_df['Model Prediction'].mean(),color='#1f497d',ls='--')\n",
    "        ax.set(ylim=(0, 1.1))\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.show()\n",
    "    \n",
    "        print('___________________________________')\n",
    "        print('mean values for header and graph')\n",
    "        display(np.round(score_df[['Model Prediction','Actual Results']].mean()*100, 2))\n",
    "        print('_______________________________________________________________________________________________________________________')\n",
    "    \n",
    "        if get_data:\n",
    "            score_df['date'] = i\n",
    "            df_all = pd.concat([df_all, score_df])\n",
    "            print(score_df)\n",
    "    # Get a retention decile plot for all probabilities together\n",
    "    actuals_avg = df_all.groupby('deciles_x')['Actual Results'].mean()\n",
    "\n",
    "    #Reformat df shape to use in plot\n",
    "    df_all = df_all.pivot(index = 'date', columns = 'deciles_x', values = 'Model Prediction')\n",
    "    print('_______________________________________________________________________________________________________________________')\n",
    "    actual_avg_df = pd.DataFrame(actuals_avg.values).T\n",
    "    actual_avg_df.columns = range(1,11)\n",
    "    actual_avg_df.index = ['Actual Results']\n",
    "    df_allplot = pd.concat([df_all, actual_avg_df])\n",
    "    df_allplot = df_allplot.reset_index()\n",
    "\n",
    "    df_allplot_long = pd.melt(df_allplot, id_vars='index', value_vars=[1,2,3,4,5,6,7,8,9,10], var_name='Decile',\n",
    "                              value_name = 'Probability')\n",
    "    fig, ax = plt.subplots(figsize = (12, 6))\n",
    "    ax= sns.lineplot(x=\"Decile\", y=\"Probability\", hue=\"index\", marker=\"D\", palette = \"Blues\", data=df_allplot_long) #'#b3d3f1'\n",
    "\n",
    "    # Get the lines from the plot\n",
    "    lines = ax.get_lines()\n",
    "    \n",
    "    # Change the color of the specific line (e.g., the second line) to orange\n",
    "    for line, label in zip(lines, ax.get_legend().get_texts()):\n",
    "        if label.get_text() == 'Actual Results':\n",
    "            line.set_color('orange')\n",
    "\n",
    "    # Update the legend to reflect the color change\n",
    "    new_labels = [text.get_text() for text in ax.get_legend().get_texts()]\n",
    "    new_colors = [line.get_color() for line in lines]\n",
    "    \n",
    "    # Remove the old legend\n",
    "    ax.get_legend().remove()\n",
    "    \n",
    "    # Create a new legend with updated colors\n",
    "    ax.legend(handles=ax.get_lines(), labels=new_labels)\n",
    "    \n",
    "    ax.set(ylim=(0.8, 1.1))\n",
    "    # plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_final_retention_decile_graph(df_merge, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brier Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: brier score from sklearn can also be used\n",
    "df_merge['diff_sqrd'] = np.subtract(df_merge.renewed, df_merge.probstm_x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Brier scores per model rundate\n",
    "df_merge.groupby(['date_x'])['diff_sqrd'].mean() #brier score per model run date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSE mean probs vs actuals per decile\n",
    "This method was done to test which process makes more sense. The below method averages the probabilities across deciles and averages actual renewal across deciles and then the mean squared error between them are calculated. This method scores were not a part of the final analysis put forward to the client because the averaging probabilities out on a decile loses valuable informatin for that decile. Instead calculating diff_squared per customer and averaging it out per a model run date (as above) reserves information of each customer and provides accurate information to compare model runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = df_merge.date_x.value_counts().index.sort_values()\n",
    "for i in dates:\n",
    "    print(i)\n",
    "    deciles = df_merge[df_merge.date_x == i].groupby('deciles_x')['probstm_x'].mean()\n",
    "    score_df = deciles.to_frame()\n",
    "    score_df['deciles'] = score_df.index\n",
    "    score_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    rfactual = df_merge.groupby('deciles_x')['renewed'].mean()\n",
    "    rfactual.reset_index(drop=True, inplace=True)\n",
    "    score_df = pd.concat((score_df, rfactual), axis = 1)\n",
    "    score_df.rename(columns={'probstm_x':\"Model Prediction\", 'renewed':'Actual Results'}, inplace=True)\n",
    "    score_df['brier_score_deciles'] = np.subtract(score_df['Actual Results'], score_df['Model Prediction'])**2\n",
    "\n",
    "    print(score_df[['deciles', 'brier_score_deciles']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def create_df_confusion_matrix(dfclv):\n",
    "    \"\"\"\n",
    "    Creates a DataFrame which will be an input to the confusion matrices.\n",
    "    The dataframew will be a subset of the main dataframe with required columns.\n",
    "\n",
    "    Creates a predicted class column which is the max(column name) out of all the probability columns and then maps\n",
    "    it out the the correct segment\n",
    "\n",
    "    Parameters:\n",
    "    dfclv (pd.DataFrame): Input DataFrame containing customer data with columns \n",
    "                          'date', 'audienceid', 'clvcustomersegment', 'probstm', \n",
    "                          'probsgb', 'probsecondary', 'probchurn', and 'deciles'.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing 'audienceid', 'predicted_class', 'actual_class', \n",
    "                  and 'date' with predicted and actual customer segments.\n",
    "    \"\"\"\n",
    "    #filter out the required columns\n",
    "    df = dfclv[['date', 'audienceid', 'clvcustomersegment', 'probstm', 'probsgb', 'probsecondary', 'probchurn', 'deciles']].copy()\n",
    "\n",
    "    #separate initial model run dates and the actual model run date\n",
    "    df_initial = df[df.date != df.date.max()]\n",
    "\n",
    "    df_initial_stm = df_initial[df_initial.clvcustomersegment == 'STM']\n",
    "\n",
    "    #map the max probability class to the segment\n",
    "    map_classes = {'probstm': 'STM', 'probsgb': 'Churn', 'probsecondary': 'Churn', 'probchurn': 'Churn'}\n",
    "    df_initial_stm[\"predicted_class\"] = df_initial_stm[['probstm', 'probsgb', 'probsecondary', 'probchurn']].idxmax(axis=1)\n",
    "    df_initial_stm[\"predicted_class\"] = df_initial_stm[\"predicted_class\"].map(map_classes)\n",
    "\n",
    "    #actuals df\n",
    "    df_comp = df[df.date == df.date.max()]\n",
    "    df_comp = df_comp[df_comp.audienceid.isin(df_initial_stm.audienceid.values.tolist())]\n",
    "\n",
    "    #merge dfs\n",
    "    \n",
    "    df_merge = df_initial_stm.merge(df_comp, how='left', on='audienceid')[['audienceid', 'predicted_class', 'clvcustomersegment_y', 'date_x']]\n",
    "    df_merge = df_merge.rename(columns={'clvcustomersegment_y': 'actual_class', 'date_x': 'date'})\n",
    "    \n",
    "    #churn where the actual class is na\n",
    "    actuals_retention_map = {'STM': 'STM', 'SGB': 'Churn', 'Secondary': 'Churn', 'Churn': 'Churn'}\n",
    "    df_merge['actual_class'] = np.where(df_merge.actual_class.isna(), 'Churn', df_merge.actual_class)\n",
    "    df_merge[\"actual_class\"] = df_merge[\"actual_class\"].map(actuals_retention_map)\n",
    "    \n",
    "    return df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confusion_matrix = create_df_confusion_matrix(dfclv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate confusion matrix for a given segment\n",
    "segments = ['STM', 'Churn']\n",
    "def generate_confusion_matrix(date):\n",
    "    actual = df_confusion_matrix[df_confusion_matrix.date == date]['actual_class']\n",
    "    predicted = df_confusion_matrix[df_confusion_matrix.date == date]['predicted_class']\n",
    "    cm = confusion_matrix(actual, predicted, labels=segments)\n",
    "    return cm\n",
    "\n",
    "dates = df_confusion_matrix.date.value_counts().index.sort_values()\n",
    "\n",
    "# Generate and plot confusion matrices for each segment\n",
    "for i in dates:\n",
    "    cm = generate_confusion_matrix(i)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=segments, yticklabels=segments)\n",
    "    plt.title(f'Confusion Matrix for {i}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Premium STM Analysis\n",
    "The below analysis is Eagles specific and has nothing to do with Model Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLV Histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "sns.set(font='Arial', style = 'white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Can add seasonyear filter\n",
    "def create_dist_graph(MAX_AXS, SERIES_1, SERIES_2=False, save_file=False, SAVEFILEPATH=''):\n",
    "    \"\"\"\n",
    "    This function creates the distribution graph shared out with clients of the segments finalclv with vertical \n",
    "    markers for central tendency.  There's an option to include a second subseries to note addition central tendency,\n",
    "    usually used for premium within the stm segment. Best practice would be to have save_file parameter set to false \n",
    "    while you adjust the max value of the X-axis until the graph looks good.  Then run again with save_file=True which\n",
    "    will save a png out to the drive. \n",
    "    inputs:\n",
    "        MAX_AXS - Due to outliers, set this value such that the graph shows an informative distribution\n",
    "        SERIES_1 - a pandas series of just finalclv, after segmentation\n",
    "        SERIES_2 - a 2nd pandas series that a subset of the first, used to mark additional cental tendency markers\n",
    "        save_file - default is false while trial and error is performed to get a reasonable looking distribution to \n",
    "            report out\n",
    "        SAVEFILEPATH - the name and path for the final png file of the distribution\n",
    "    outputs:\n",
    "        - display of graph, various stats, and a png file written out to a specified location\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    #get obs cnt to show how many are off the chart after adjusting max_axs parm\n",
    "    obs_cnt = SERIES_1.count()\n",
    "    off_chart_obs_cnt = SERIES_1[SERIES_1 > MAX_AXS].count()\n",
    "        \n",
    "    print('slide detail to add to final slide')\n",
    "    print('Series_1')\n",
    "    print('_'*20)\n",
    "    print('Total:\\t',\"{:,}\".format(np.round(SERIES_1.sum(),2)))\n",
    "    print('Median:\\t',\"{:,}\".format(np.round(SERIES_1.median(),2)))\n",
    "    print('Mean:\\t',\"{:,}\".format(np.round(SERIES_1.mean(),2)))\n",
    "    print('Max:\\t',\"{:,}\".format(np.round(SERIES_1.max(),2)))\n",
    "    print('Count Obs > Max_Axs:\\t',\"{:,} {:,}%\".format(off_chart_obs_cnt, np.round(off_chart_obs_cnt/obs_cnt*100,2)))\n",
    "    print('_'*40)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize = (12, 6))\n",
    "    sns.distplot(SERIES_1, kde = False, bins = np.linspace(0, MAX_AXS, 50), \\\n",
    "                 color = '#1F497D', hist_kws=dict(alpha = 1, linewidth = 0))\n",
    "    sns.distplot(SERIES_2, kde = False, bins = np.linspace(0, MAX_AXS, 50), \\\n",
    "                 color = 'turquoise', hist_kws=dict(alpha = 1, linewidth = 0))\n",
    "    sns.despine()\n",
    "    ax.set_xlim(0, MAX_AXS)\n",
    "    \n",
    "    # vertical flags for central tendency\n",
    "    ax.axvline(x=SERIES_1.mean(), color = '#7ea7f0')\n",
    "    ax.axvline(x=SERIES_1.median(), color = '#7ea7f0')\n",
    "    \n",
    "    # vertical flags for central tendency if 2nd series exists\n",
    "    if type(SERIES_2) == pd.core.series.Series:\n",
    "        \n",
    "        print('Series_2')\n",
    "        print('_'*20)\n",
    "        print('Total:\\t',\"{:,}\".format(np.round(SERIES_2.sum(),2)))\n",
    "        print('Median:\\t',\"{:,}\".format(np.round(SERIES_2.median(),2)))\n",
    "        print('Mean:\\t',\"{:,}\".format(np.round(SERIES_2.mean(),2)))\n",
    "        print('_'*40)\n",
    "    \n",
    "        ax.axvline(x=SERIES_2.mean(), color = 'grey')\n",
    "        ax.axvline(x=SERIES_2.median(), color = 'grey')\n",
    "        \n",
    "    #finish formatting graph\n",
    "    ax.set_xlabel(\"3 Year Predicted CLV\", fontsize = 14)\n",
    "    ax.set_ylabel(\"Number of Customers\", fontsize = 14)\n",
    "    xlabels = ['$' + '{:,.0f}'.format(x) for x in ax.get_xticks()]\n",
    "    ax.set_xticklabels(xlabels)\n",
    "    plt.tick_params(labelsize = 14)\n",
    "    \n",
    "    if save_file:\n",
    "        print('saving file to:  ',SAVEFILEPATH)\n",
    "        plt.savefig(SAVEFILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEGMENT = 'STM'\n",
    "\n",
    "seg_finalclv = dfclv[(dfclv.clvcustomersegment==SEGMENT) & (dfclv.currentpremium==0) & (dfclv.modelrundate == '2024-02-12 00:00:00.000000000 -05:00')]['finalclv']\n",
    "subseg_finalclv = dfclv[(dfclv.clvcustomersegment==SEGMENT) & (dfclv.currentpremium==1) & (dfclv.modelrundate == '2024-02-12 00:00:00.000000000 -05:00')]['finalclv']\n",
    "\n",
    "create_dist_graph(MAX_AXS=90000, SERIES_1=seg_finalclv, SERIES_2=subseg_finalclv, save_file=False, SAVEFILEPATH= '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfclv[dfclv.finalclv == seg_finalclv.max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Violin Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating spend buckets\n",
    "df = dfclv[(dfclv.clvcustomersegment == 'STM') & (dfclv.modelrundate == '2024-02-12 00:00:00.000000000 -05:00')].copy()\n",
    "\n",
    "# Creating spend buckets\n",
    "bins = [800, 2000, 3000, 4000, 5000, 6000, 8000]\n",
    "labels = [f'{bins[i]}-{bins[i+1]}' for i in range(len(bins)-1)]\n",
    "df['spend_bucket'] = pd.cut(df['totaleaglesspend'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# Creating the violin plot\n",
    "g = sns.catplot(data=df, x = 'spend_bucket', hue = 'currentpremium'\n",
    "                , y='finalclv', kind='violin', height=6.45, aspect=1.55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating spend buckets\n",
    "df = dfclv[(dfclv.clvcustomersegment == 'STM') & (dfclv.modelrundate == '2024-02-12 00:00:00.000000000 -05:00') & (dfclv.finalclv < 200000)].copy()\n",
    "\n",
    "# Creating spend buckets\n",
    "bins = [0, 0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 1]\n",
    "labels = [f'{bins[i]}-{bins[i+1]}' for i in range(len(bins)-1)]\n",
    "df['utilization_bucket'] = pd.cut(df['utilizationrate'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# Creating the violin plot\n",
    "g = sns.catplot(data=df, x = 'utilization_bucket', hue = 'currentpremium'\n",
    "                , y='finalclv', kind='violin', height=6.45, aspect=1.55)\n",
    "\n",
    "# Setting y-axis limits to ensure no negative values are shown\n",
    "# g.set(ylim=(-1, df['finalclv'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating spend buckets\n",
    "df = dfclv[(dfclv.clvcustomersegment == 'STM') & (dfclv.modelrundate == '2024-02-12 00:00:00.000000000 -05:00') & (dfclv.finalclv < 200000)].copy()\n",
    "\n",
    "# Creating spend buckets\n",
    "bins = [0, 10, 50, 100, 200, 6940]\n",
    "labels = [f'{bins[i]}-{bins[i+1]}' for i in range(len(bins)-1)]\n",
    "df['ticketstransferred_bucket'] = pd.cut(df['ticketstransferred'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# Creating the violin plot\n",
    "g = sns.catplot(data=df, x = 'ticketstransferred_bucket', hue = 'currentpremium'\n",
    "                , y='finalclv', kind='violin', height=6.45, aspect=1.55)\n",
    "\n",
    "# Setting y-axis limits to ensure no negative values are shown\n",
    "# g.set(ylim=(-1, df['finalclv'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
