{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold, cross_validate\n",
    "# from sklearn.model_selection import learning_curve, validation_curve, cross_val_score, ShuffleSplit\n",
    "# from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "# from sklearn.impute import SimpleImputer, KNNImputer\n",
    "# from sklearn.preprocessing import RobustScaler, OneHotEncoder, LabelEncoder\n",
    "# from sklearn.compose import ColumnTransformer, make_column_selector,make_column_transformer\n",
    "\n",
    "# from sklearn.metrics import classification_report, roc_curve, brier_score_loss, roc_auc_score, f1_score\n",
    "# from sklearn.metrics import make_scorer, recall_score, matthews_corrcoef, fbeta_score\n",
    "# from sklearn import metrics\n",
    "\n",
    "# from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "# from sklearn.impute import IterativeImputer\n",
    "\n",
    "# from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "\n",
    "# import category_encoders as ce\n",
    "\n",
    "# from functools import partial\n",
    "# import plotly\n",
    "# import optuna\n",
    "\n",
    "# from missingno import missingno as mn\n",
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# from sklearn import svm, datasets\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "# from sklearn.feature_selection import RFECV\n",
    "# from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "#========================\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "#for pipeline viz\n",
    "# from sklearn import set_config                      # to change the display\n",
    "# from sklearn.utils import estimator_html_repr       # to save the diagram into HTML format\n",
    "# from IPython.core.display import display, HTML      # to visualize pipeline\n",
    "\n",
    "#do we need these with pickles?\n",
    "# import lightgbm as lgb\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import pickle # changed to pickle from pickle5\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# set_config(display='diagram')\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_col_names(df):\n",
    "    '''\n",
    "    takes a df and converts column names to lowercase underscore seperated\n",
    "    '''\n",
    "    df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# These must be as lists\n",
    "SEGMENT = ['Secondary','SGB','STM'] \n",
    "FITYEARS = [2019,2021]\n",
    "\n",
    "#for final graphs and calcs\n",
    "RETENT_YEARS = [2019,2021]\n",
    "CLV_YEAR = [2021]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the localand drive path for the performance analysis project\n",
    "LOCAL_DAT_PATH = r'C:\\Users\\rhead\\Documents\\local_ds\\Model_refreshes\\eagles\\\\'\n",
    "\n",
    "DRIVE_PATH = r'C:\\Users\\rhead\\Documents\\local_ds\\Model_refreshes\\eagles\\\\'\n",
    "\n",
    "DT_STAMP = datetime.now().strftime(\"%y-%m-%d_%I%M%p\")\n",
    "DT_STAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check connection\n",
    "* if paths prints, good-to-go!  If not, check to make sure you are connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in os.walk(DRIVE_PATH):\n",
    "    print(x[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the data loaded here should match with what was loaded in the previous communication notebook\n",
    "dat = pd.read_csv(LOCAL_DAT_PATH+r'EaglesQA_Training_data_0117.csv')\n",
    "\n",
    "dat = fix_col_names(dat)\n",
    "\n",
    "dat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat['customersegment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dat.shape)\n",
    "dat = dat[dat['customersegment'].isin(SEGMENT)]\n",
    "print(dat.shape)\n",
    "\n",
    "dat.customersegment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preshuffle the data, mixing multiple years together\n",
    "#create copy to only need to load once (be aware it doubles Python memory reserve) \n",
    "df = dat.sample(frac=1,random_state=222).reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Segment_num_dct = {'STM': 3\n",
    "                   , 'SGB': 2\n",
    "                   , 'Secondary': 1\n",
    "                   , 'Churn': 0}\n",
    "\n",
    "df['target'] = df['customersegmentfollowingyear'].map(Segment_num_dct)\n",
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'target'\n",
    "\n",
    "df[target] = df[target].astype(int)\n",
    "\n",
    "target_labels = list(set(df[target].unique())) #used to ensure correct plots later\n",
    "print('target_labels', target_labels)\n",
    "print('target nulls:',df[target].isnull().sum())\n",
    "\n",
    "# Percent of labels\n",
    "np.round(df[target].value_counts()/len(df[target])*100,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any date columns needing to be coverted to datetime\n",
    "\n",
    "ENCODE_DATES = False\n",
    "if ENCODE_DATES:\n",
    "\n",
    "    def process_time_features(df):\n",
    "        date_cols = [x for x in df.columns if 'date' in x]\n",
    "        print(date_cols)\n",
    "        for name in date_cols:\n",
    "            df[name] = pd.to_datetime(df[name])\n",
    "\n",
    "        df.drop(date_cols,axis=1,inplace=True) #note this is dropping, comment out to keep\n",
    "        \n",
    "        return df\n",
    "\n",
    "    df = process_time_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Any ordinal features in the dataset\n",
    "\n",
    "ENCODE_ORD = False\n",
    "if ENCODE_ORD:\n",
    "    \n",
    "    def encode_ordinal_features(df):\n",
    "\n",
    "#         df['ownrent'] = df['ownrent'].map({ 'Definite Renter': 0\n",
    "#                                             , 'Probable Renter': 0\n",
    "#                                             , 'Definite Owner': 1\n",
    "#                                             , 'Probable Owner': 1\n",
    "#                                             })\n",
    "#         df['education'] = df['education'].map({'Vocational/Technical School': 0\n",
    "#                                                , 'High School': 1\n",
    "#                                                , 'College': 2\n",
    "#                                                , 'Graduate School': 3\n",
    "#                                               })\n",
    "#         df['maritalstatus'] = df['maritalstatus'].map({'Definitely Single': 1\n",
    "#                                            , 'Possibly Single':1\n",
    "#                                            , 'Possibly Married':2\n",
    "#                                            , 'Definitely Married':2\n",
    "#                                           })\n",
    "#         df['householdincome'] = df['householdincome'].map({'$0-$15,000': 0\n",
    "#                                                  ,'$15,001-$20,000': 1\n",
    "#                                                  ,'$20,001-$30,000': 2\n",
    "#                                                  ,'$30,001-$40,000': 3\n",
    "#                                                  ,'$40,001-$50,000': 4\n",
    "#                                                  ,'$50,001-$60,000': 5\n",
    "#                                                  ,'$60,001-$75,000': 6\n",
    "#                                                  ,'$75,001-$100,000': 7\n",
    "#                                                  ,'$100,001-$125,000': 8\n",
    "#                                                  ,'$125,001-$150,000': 9\n",
    "#                                                  ,'$150,001+': 10\n",
    "#                                                 })\n",
    "#         df['demographicsgender'] = df['demographicsgender'].map({'Male': -1\n",
    "#                                            , 'Neutral':1\n",
    "#                                            , 'Female':2\n",
    "#                                           })\n",
    "        return df\n",
    "\n",
    "    df = encode_ordinal_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical - Address Long Tail\n",
    "* CLV_GRAPH Note - Only copy the final dictionaries over to the notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check label counts of categorical features\n",
    "#TODO - CREATE CLASS OF PREPROCESS FUNCTON TO INCLUDE IN PIPELINE\n",
    "\n",
    "cat_ls = [\n",
    "\n",
    "'acquisitionsource',\n",
    "\n",
    "]\n",
    "\n",
    "for c in cat_ls:\n",
    "    print('================================================')\n",
    "    print(c)\n",
    "    print('________________________________________________')\n",
    "    print(df[c].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========== Setup ======================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Out SeasonYears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_years = list(set(FITYEARS+RETENT_YEARS+CLV_YEAR))\n",
    "\n",
    "#create renewed flag\n",
    "print(df.shape)\n",
    "df = df[df['seasonyear'].isin(get_years)].copy()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n distribution of customersegment by year \\n','_'*30)\n",
    "pd.crosstab(df.customersegment, df.seasonyear, normalize='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate renew\n",
    "df['renewed'] = (df['customersegment'] == df['customersegmentfollowingyear']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n actual renewal rate by segment and by year \\n','_'*30)\n",
    "pd.crosstab(df.customersegment, df.seasonyear,aggfunc='mean', values=df.renewed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------- Model Predictions ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Variables for Each Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See model prep notebook for feature selection and model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Select Final variables for Models\n",
    "stm_variables = [\n",
    " 'acquisitionsource',\n",
    " 'attendancerateyoydelta',\n",
    " 'avgprimaryeaglesspend',\n",
    " 'avgprimarypurchasedaysbeforeevent',\n",
    " 'avgpurchasedaysbeforeevent',\n",
    " 'avgsecondaryeaglesspend',\n",
    " 'avgsecondarypurchasedaysbeforeevent',\n",
    " 'avgsecondaryrevenue',\n",
    " 'distancetovenue',\n",
    " 'eagleseventattendancerate',\n",
    " 'eaglesotherattendance',\n",
    " 'eaglesseatsattendedrate',\n",
    " 'eagleswinningattendanceregseasonrate',\n",
    " 'email_clickrate_1yr',\n",
    " 'email_clickrate_30days',\n",
    " 'email_clickrate_30to60days',\n",
    " 'email_clickrate_60days',\n",
    " 'email_clickrate_60to90days',\n",
    " 'email_clickrate_90days',\n",
    " 'email_clickrate_regseason',\n",
    " 'email_openrate_1yr',\n",
    " 'email_openrate_30days',\n",
    " 'email_openrate_30to60days',\n",
    " 'email_openrate_60days',\n",
    " 'email_openrate_60to90days',\n",
    " 'email_openrate_90days',\n",
    " 'email_openrate_regseason',\n",
    " 'lffattendance',\n",
    " 'lostpercent',\n",
    " 'marketingattendance',\n",
    " 'membersince',\n",
    " 'nextYearSTMContractEnd',\n",
    " 'noneaglesgamesattendance',\n",
    " 'nonplaneaglesprimaryspend',\n",
    " 'primaryeaglesspend',\n",
    " 'primarypostseasoneaglesspend',\n",
    " 'realizedvalue',\n",
    " 'secondaryeaglesspend',\n",
    " 'spendyoy',\n",
    " 'suiteeventsattendance',\n",
    " 'templeattendance',\n",
    " 'totalsecondaryrevenue',\n",
    " 'utilizationrate',\n",
    " 'ytd_eaglesotherrevenue',\n",
    " 'ytd_lffrevenue',\n",
    " 'ytd_marketingrevenue',\n",
    " 'ytd_noneaglesrevenue',\n",
    " 'ytd_suiteeventsrevenue',\n",
    " 'ytd_templerevenue'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with variables for modeling each customer type\n",
    "df_stm = df[df['customersegment'] == 'STM'].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read In Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUILD_PATH = DRIVE_PATH\n",
    "print(BUILD_PATH)\n",
    "\n",
    "#get list of files in directory\n",
    "os.listdir(BUILD_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Consolidate to one pickle file\n",
    "STM_MODEL_FILENAME = r'2019_2021_eagles_finalized_clv_STM_model_23-02-15_0103PM.p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BUILD_PATH+STM_MODEL_FILENAME, 'rb') as input_file:\n",
    "    stm_model = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the n_jobs value\n",
    "In the loaded environment the pickle files created in an older environment were not able to load properly and errored out on the n_jobs parameter as it was not able to parallelize the model. We had to make this bandaid fix to run the model in the new environment by changing the n_jobs parameter to 1 from -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the pipeline to understand its structure\n",
    "print(stm_model)\n",
    "\n",
    "# Assuming the preprocessing step is named 'preprocess' and the classifier is named 'classifier'\n",
    "preprocess_pipeline = stm_model.named_steps['preprocess_pipeline']\n",
    "calibrated_classifier = stm_model.named_steps['calib_clf']\n",
    "\n",
    "# Check if the preprocess pipeline and classifier have the n_jobs parameter\n",
    "# Change n_jobs to 1 if it exists\n",
    "if hasattr(preprocess_pipeline, 'n_jobs'):\n",
    "    preprocess_pipeline.n_jobs = 1\n",
    "    print(f\"n_jobs in the preprocess pipeline is now set to: {preprocess_pipeline.n_jobs}\")\n",
    "\n",
    "if hasattr(calibrated_classifier, 'n_jobs'):\n",
    "    calibrated_classifier.n_jobs = 1\n",
    "    print(f\"n_jobs in the classifier is now set to: {calibrated_classifier.n_jobs}\")\n",
    "    \n",
    "# print(stm_model)\n",
    "\n",
    "# Optionally, save the modified pipeline back to a pickle file\n",
    "with open(BUILD_PATH+'2019_2021_eagles_finalized_clv_STM_model_7312024.p', 'wb') as file:\n",
    "    pickle.dump(stm_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model again after changing the parameter\n",
    "with open(BUILD_PATH+'2019_2021_eagles_finalized_clv_STM_model_7312024.p', 'rb') as input_file:\n",
    "    stm_model = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on STM Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = ['ProbChurn', 'ProbSecondary', 'ProbSGB', 'ProbSTM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STM\n",
    "predictions_stm = stm_model.predict_proba(df_stm[stm_variables])\n",
    "predictions_stm_df = pd.DataFrame(predictions_stm, columns = pred_labels, index = df_stm.index)\n",
    "predictions_stm_df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------- Retention Slides ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retention Decile Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_final_retention_decile_graph(df_in, SEGMENT, get_data=False):\n",
    "    \"\"\"\n",
    "    Displays retention graph for final slide deck along with additional labels for the mean\n",
    "    probs and actuals. \n",
    "    \"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (12, 6))\n",
    "\n",
    "    ### Create Deciles\n",
    "    df_in['Deciles'] = pd.Series(pd.qcut(df_in['Prob'+SEGMENT], 10, labels=False, duplicates='drop'))\n",
    "\n",
    "    # Create subsets grouping by the different deciles\n",
    "    deciles = df_in.groupby('Deciles')['Prob'+SEGMENT].mean()\n",
    "    score_df = deciles.to_frame()\n",
    "    score_df['Deciles'] = score_df.index + 1\n",
    "    score_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    rfactual = df_in.groupby('Deciles')['renewed'].mean()\n",
    "    score_df = pd.concat((score_df, rfactual), axis = 1)\n",
    "    score_df.rename(columns={'Prob'+SEGMENT:\"Model Prediction\", 'renewed':'Actual Results'},inplace=True)\n",
    "   \n",
    "    #Reformat df shape to use in plot\n",
    "    df_forplot = pd.melt(score_df, id_vars=['Deciles'], value_vars=['Model Prediction', 'Actual Results'], var_name='Model', value_name = 'Probability')\n",
    "    \n",
    "    ### Create Plot\n",
    "    ax= sns.lineplot(x=\"Deciles\", y=\"Probability\", hue=\"Model\",\n",
    "                      palette= ['#1f497d', '#4290dc'], marker=\"D\", data=df_forplot) #'#b3d3f1'\n",
    "    plt.axhline(y=score_df['Actual Results'].mean(), color='#4290dc',ls='--')\n",
    "    plt.axhline(y=score_df['Model Prediction'].mean(),color='#1f497d',ls='--')\n",
    "    ax.set(ylim=(0, 1.1))\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "    #Note - Previously pasted the following data into slide    \n",
    "    # score_df = score_df[['Deciles','Model Prediction','Actual Results']]\n",
    "    # print('___________________________________')\n",
    "    # print('data to copy into powerpoint graph')\n",
    "    # display(score_df)\n",
    "    print('___________________________________')\n",
    "    print('mean values for header and graph')\n",
    "    display(np.round(score_df[['Model Prediction','Actual Results']].mean()*100,2))\n",
    "\n",
    "    if get_data:\n",
    "        return score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO retain the season year from the initial model build communication graphs notebook that was put up in the model readout deck. For\n",
    "# Eagles we compared the 2021 season year results. Hence, to calculate the Brier score for that year we will use the formula in the next cell\n",
    "create_final_retention_decile_graph(df_stm_output[df_stm_output.seasonyear==2021].copy(), SEGMENT='STM', get_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brier Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brier = df_stm_output[['audienceid', 'renewed', 'ProbSTM']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brier['brier_score_raw'] = np.subtract(df_brier.renewed, df_brier.ProbSTM)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brier['brier_score_raw'].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
