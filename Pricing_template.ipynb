{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import yeojohnson\n",
    "from scipy.stats import skew\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.image as mpimg\n",
    "from scipy.stats import normaltest\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold, cross_validate, learning_curve, RepeatedKFold\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PowerTransformer, FunctionTransformer, RobustScaler, StandardScaler\n",
    "from sklearn.linear_model import BayesianRidge, Lasso\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import shapiro\n",
    "from mapie.regression import MapieRegressor\n",
    "from mapie.metrics import regression_coverage_score\n",
    "\n",
    "\n",
    "import scipy.stats as ss\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "import math \n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "import os\n",
    "import re\n",
    "import timeit\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "\n",
    "from mapie.metrics import regression_coverage_score\n",
    "\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "\n",
    "pd.options.display.float_format = '{:.9f}'.format\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_col_names(df):\n",
    "    '''\n",
    "    takes a df and converts column names to lowercase underscore seperated\n",
    "    '''\n",
    "    df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(df,data_type='numeric'):\n",
    "    '''\n",
    "    Calcualte the descriptive stats for different type of data\n",
    "        Args:\n",
    "            df: DataFrame.\n",
    "            data_type=['numeric','categorical']\n",
    "        Returns:\n",
    "            this function returns a data frame containing summary stats for different types of data\n",
    "                \n",
    "\n",
    "    '''\n",
    "    df_column=df.select_dtypes(include=['int64','float64']).columns\n",
    "    df_describe = df[df_column]\n",
    "    stats = {\n",
    "    \"Variable\": list(x.title() for x in df_column),\n",
    "    \"Min\": list(df_describe.apply(np.min,axis=0).values),\n",
    "    \"Median\": list(df_describe.apply(np.median,axis=0).values),\n",
    "    \"Mean\": list(df_describe.apply(np.mean,axis=0).values),\n",
    "    \"Variance\": list(df_describe.apply(np.var,axis=0).values),\n",
    "    \"Max\": list(df_describe.apply(np.max,axis=0).values),\n",
    "    \"Std\": list(df_describe.apply(np.std,axis=0).values),\n",
    "    \"Kurtosis\": df_describe.apply(lambda x:x.kurt(),axis=0).values.tolist(),\n",
    "    \"Skewness\": df_describe.apply(lambda x:x.skew(),axis=0).values.tolist(),\n",
    "    # \"Sum\": list(df_describe.apply(np.sum,axis=0).values),\n",
    "    # \"Mad\": df_describe.apply(lambda x:x.mad(),axis=0).values.tolist(),\n",
    "    \"N_Zeros\": df_describe.apply(lambda x:len(x)-np.count_nonzero(x),axis=0).values.tolist(),\n",
    "    \"N_Nulls\": df_describe.apply(lambda x:np.count_nonzero(np.isnan(x)),axis=0).values.tolist(),\n",
    "    'Count': df_describe.apply(lambda x:len(x),axis=0).values\n",
    "    }\n",
    "    print (\"Numeric Variables Dataset Shape: \"+ str(df_describe.shape))\n",
    "    return pd.DataFrame(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_descriptive_stats(df_in, col_ls, nfeats_per_group=25):\n",
    "    \"\"\"\n",
    "    This function displays descriptive statistics for the specified columns/list of columns. It splits the specified columns into groups and displays \n",
    "    the statistics for each group side by side.\n",
    "    \n",
    "    Parameters:\n",
    "    df_in (pd.DataFrame): The input DataFrame containing the data for which descriptive statistics are to be displayed.\n",
    "    col_ls (list): A list of column names for which the descriptive statistics are to be calculated and displayed.\n",
    "    nfeats_per_group (int, optional): The number of features to display per group. Default is 25.\n",
    "    \n",
    "    Returns:\n",
    "    None: The function displays the results and does not return any value.\n",
    "    \n",
    "    The function performs the following steps:\n",
    "    1. Prints the count of columns to be analyzed.\n",
    "    2. Iterates over each group of columns, calculates descriptive statistics for each group, and displays the \n",
    "    statistics in three categories:\n",
    "    - General statistics including Min, Median, Mean, Max, Kurtosis, and Skewness.\n",
    "    - Standard Deviation.\n",
    "    - Count statistics including the number of zeros, number of nulls, and total count.\n",
    "    5. Uses pandas Styler to format the DataFrames and apply color gradients to enhance visual interpretation.\n",
    "    \"\"\"\n",
    "    \n",
    "    print('cols cnt for analysis:  ', len(col_ls))\n",
    "    \n",
    "    def horizontal(dfs):\n",
    "        from IPython.display import HTML\n",
    "        \n",
    "        html = '<div style=\"display:flex\">'\n",
    "        for df in dfs:\n",
    "            html += '<div style=\"margin-right: 32px\">'\n",
    "            html += df.to_html()\n",
    "            html += '</div>'\n",
    "        html += '</div>'\n",
    "        display(HTML(html))\n",
    "    \n",
    "    loop_cnt = range(int(np.ceil(len(col_ls)/nfeats_per_group)))\n",
    "    print('loop cnt: ',loop_cnt)\n",
    "    print('feature count per table: ',nfeats_per_group)\n",
    "\n",
    "    \n",
    "    n=0\n",
    "    for i in loop_cnt:\n",
    "        \n",
    "        tbl_cols = col_ls[n:n+nfeats_per_group]\n",
    "\n",
    "        \n",
    "        df_num_stats = summary(df_in[tbl_cols])\n",
    "\n",
    "        df_stat_points = df_num_stats[['Variable','Min', 'Median', 'Mean', 'Max', 'Kurtosis', 'Skewness']]\n",
    "        # df_stat_points.index = df_num_stats['Variable']\n",
    "        df_stat_std = df_num_stats[['Std']]\n",
    "        df_stat_cnts = df_num_stats[['N_Zeros',\t'N_Nulls', 'Count']]\n",
    "        \n",
    "        # display(df_num_stats_summary, '\\n')\n",
    "        \n",
    "        # df_time_med = df_time_pivot.loc[:, pd.IndexSlice['median',:]]\n",
    "        # df_time_mean = df_time_pivot.loc[:, pd.IndexSlice['mean',:]]\n",
    "       \n",
    "        horizontal(\n",
    "\n",
    "            # https://stackoverflow.com/questions/38931566/pandas-style-background-gradient-both-rows-and-columns\n",
    "            # df_stat_points - might benefit from all on the same scale, but there's no direct option, this link gives a possible solution\n",
    "            # other option is to have coloring follow a log scale to remove influence of outliers\n",
    "            \n",
    "           [df_stat_points.style.format(precision=2,thousands=',').background_gradient(cmap='PuBu',axis=1),\n",
    "            df_stat_std.style.format(precision=2,thousands=',').hide(axis=\"index\").background_gradient(cmap='inferno',axis=1),\n",
    "            df_stat_cnts.style.format(precision=2,thousands=',').hide(axis=\"index\").background_gradient(cmap='BuGn',axis=1)]\n",
    "            # raw=True\n",
    "        )\n",
    "\n",
    "        n+=nfeats_per_group\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_normality_and_style(df, num_ls):\n",
    "    \"\"\"\n",
    "    Checks the normality of columns in a DataFrame and returns a styled DataFrame\n",
    "    highlighting columns that are not normally distributed.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the data.\n",
    "    num_ls (list): List of column names to check for normality.\n",
    "\n",
    "    Returns:\n",
    "    pd.io.formats.style.Styler: A styled DataFrame with normality test results.\n",
    "    \"\"\"\n",
    "    # Create a list to store the results\n",
    "    results = []\n",
    "\n",
    "    # Loop through the columns and perform the normality test\n",
    "    for i in num_ls:\n",
    "        stat, p_value = normaltest(df[i].values)\n",
    "        label = \"Not Normal\" if p_value >= 0.05 else \"Normal\"\n",
    "        results.append([i, label, stat, p_value])\n",
    "\n",
    "    # Convert the results list to a DataFrame\n",
    "    results_df = pd.DataFrame(results, columns=['Column', 'Normality', 'Statistic', 'p-value'])\n",
    "\n",
    "    # Function to highlight Gaussian rows\n",
    "    def highlight_gaussian(row):\n",
    "        color = 'lightblue' if row['Normality'] == 'Not Normal' else ''\n",
    "        return ['background-color: {}'.format(color) for _ in row]\n",
    "\n",
    "    # Apply the highlighting function to the DataFrame\n",
    "    styled_results_df = results_df.style.apply(highlight_gaussian, axis=1)\n",
    "\n",
    "    return styled_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_jointplots(df, num_ls, target):\n",
    "    \"\"\"\n",
    "    Generates joint plots for each numerical feature in the list against the target variable,\n",
    "    including both the original and Yeo-Johnson transformed features.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the data.\n",
    "    num_ls (list): List of numerical column names to plot.\n",
    "    target (str): The target variable name.\n",
    "    \n",
    "    \"\"\"\n",
    "    for variable in num_ls:\n",
    "        print(\"_\" * 30)\n",
    "        print(variable)\n",
    "        \n",
    "        # Normal feature plot (g0)\n",
    "        g0 = sns.jointplot(x=df[variable], y=df[target], kind='reg')\n",
    "        g0.savefig('g0.png')\n",
    "        plt.close(g0.fig)\n",
    "\n",
    "        try:\n",
    "            # Yeo-Johnson transformed feature plot (g1)\n",
    "            transformed_variable, _ = yeojohnson(df[variable])\n",
    "            g1 = sns.jointplot(x=transformed_variable, y=df[target], kind='reg')\n",
    "            g1.savefig('g1.png')\n",
    "            plt.close(g1.fig)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            g1 = None\n",
    "\n",
    "        # Create subplots from temporal images\n",
    "        f, axarr = plt.subplots(ncols=2, sharey=False, figsize=(25, 30))\n",
    "        axarr[0].imshow(mpimg.imread('g0.png'))\n",
    "        if g1:\n",
    "            axarr[1].imshow(mpimg.imread('g1.png'))\n",
    "        else:\n",
    "            axarr[1].text(0.5, 0.5, 'Transformation Error', \n",
    "                          horizontalalignment='center', \n",
    "                          verticalalignment='center')\n",
    "\n",
    "        # Turn off x and y axis\n",
    "        [ax.set_axis_off() for ax in axarr.ravel()]\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Clean up the temporary image files\n",
    "        os.remove('g0.png')\n",
    "        if g1:\n",
    "            os.remove('g1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Change the data path and the target if neccesary\n",
    "# DATPATH = r'C:\\Users\\rhead\\Documents\\local_ds\\Pricing\\\\'\n",
    "\n",
    "# target = 'avg_single_game_ticket_price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Replace the name of the csv here\n",
    "# dat = fix_col_names(pd.read_csv(DATPATH+r'Regression Model Data 2023.csv').set_index('Team'))\n",
    "\n",
    "# # Output of this will specify (rows, columns) of the dataset\n",
    "# dat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the dtypes of the data set and make a note of all that migh trequire a change in dtypes\n",
    "dat.info(verbose=1,show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe to only include numberic columns for now\n",
    "df_column=dat.select_dtypes(include=['int64','float64']).columns\n",
    "df = dat[df_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correct the variable names. Remove parantheses and trailing underscores\n",
    "print(df.columns)\n",
    "df = remove_brackets_from_column_names(df)\n",
    "print(\"\\nDataFrame after removing brackets from column names:\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode and Analyze the target\n",
    "\n",
    "The target is transformed using boxcox transformation to analyze the normal distribution with the other independent variables. Transforming it via boxcox for EDA allows to easily interpret the relationship between the target and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the target and transform it\n",
    "\n",
    "df['tranformed_target'], _ = boxcox(df[target])\n",
    "\n",
    "fig,ax = plt.subplots(1,3,figsize=(20,5))\n",
    "df['tranformed_target'].plot(kind=\"hist\",ax=ax[0])\n",
    "df['tranformed_target'].plot(kind=\"kde\",ax=ax[1])\n",
    "df['tranformed_target'].plot(kind=\"box\",ax=ax[2])\n",
    "plt.show()\n",
    "print(f'{target}: {\"Not Gaussian\" if normaltest(df[target].values,)[1] < 0.05 else \"Gaussian\"}  {normaltest(df[target].values)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for unique values. Get rid of constants in the model\n",
    "df.nunique().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data type lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_columns\n",
    "num_ls = ['capacity',\n",
    "          'cost_of_living_index',\n",
    "          'per_capita_personal_income',\n",
    "          'city_population',\n",
    "          'metro_population',\n",
    "          'venue_distance_from_city_center',\n",
    "          '22-23_win_%',\n",
    "          '3yr_win_%',\n",
    "          '5_yr_win_%',\n",
    "          'attendance_rate',\n",
    "          'age_of_venue',\n",
    "          'age_of_club',\n",
    "          'forbes_valuation_2022',\n",
    "          'forbes_valuation',\n",
    "          'allstars_roster',\n",
    "          'allstars_div_roster',\n",
    "          'allstars_current',\n",
    "          'allstars_div_current',\n",
    "          'in_market_teams',\n",
    "          'former_mvp',\n",
    "          'size_of_dma',\n",
    "          'household_income',\n",
    "          'population_growth_',\n",
    "          'no_of_fortune_500_companies',\n",
    "          'average_money_spent_on_sports',\n",
    "          'job_posting_activity',\n",
    "          'student_population_percent',\n",
    "          'median_home_value',\n",
    "          'per_cap_inc_chg']\n",
    "#boolean columns to be treated separately\n",
    "bool_ls = [\n",
    "    'most_followed_team',\n",
    "    'most_watched_sport',\n",
    "    'allstars_current_flag',\n",
    "    'allstars_roster_flag',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this list should have all the columns not required for modeling\n",
    "set(df)-set(num_ls)-set(bool_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display descreptive stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare median and means. if there is a big difference probability is there for outliers. Check for number of zeroes and nulls in the data\n",
    "display_descriptive_stats(df, num_ls, nfeats_per_group=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Normality\n",
    "\n",
    "Because we have so many distributions that are not normal we will have to check for relation with the target using Yeo-Johnson transformation of the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styled_results = check_normality_and_style(df, num_ls)\n",
    "styled_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize relationship with the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use these graphs to see the relationship between the target and the feature. The graphs on the right shows the transformed feature with the target. The histograms on each side represent\n",
    "# the distribution of the fetures. Yeo-Johnson transformation will transform non-normal distribution to normal distribution to understand the relationship with the dependent variable\n",
    "plot_jointplots(df, num_ls, 'tranformed_target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------End of EDA------------------------------ #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check normality\n",
    "def check_normality(X):\n",
    "    return np.array([shapiro(X[:, i])[1] > 0.05 for i in range(X.shape[1])])\n",
    "\n",
    "# Custom transformer to apply Yeo-Johnson only to non-normally distributed columns\n",
    "class ConditionalTransformer(PowerTransformer):\n",
    "    def fit(self, X, y=None):\n",
    "        self.normality_mask_ = check_normality(X)\n",
    "        return super().fit(X[:, ~self.normality_mask_], y)\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        if hasattr(self, 'normality_mask_'):\n",
    "            X_transformed[:, ~self.normality_mask_] = super().transform(X[:, ~self.normality_mask_])\n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "\n",
    "def cross_val_tune_regressor(dfX_train, dfy_train, num_ls, bool_ls, Regressor, param_grid):\n",
    "    # Define numerical and boolean pipelines\n",
    "    num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')), \n",
    "        ('robust', RobustScaler()),\n",
    "        ('yeojohnson', FunctionTransformer())  # Placeholder for ConditionalTransformer\n",
    "    ])\n",
    "\n",
    "    bool_pipeline = Pipeline([\n",
    "        ('identity', FunctionTransformer())\n",
    "    ])\n",
    "\n",
    "    # Combine pipelines with ColumnTransformer\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', num_pipeline, num_ls),\n",
    "        ('bool', bool_pipeline, bool_ls)\n",
    "    ])\n",
    "\n",
    "    # Combine preprocessor with model in a pipeline\n",
    "    model_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', Regressor())\n",
    "    ])\n",
    "#RD: try repeated k fold. \n",
    "    # Initialize KFold\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "    # Lists to store metrics and predictions\n",
    "    in_sample_metrics = []\n",
    "    out_of_sample_metrics = []\n",
    "    out_of_sample_predictions = np.zeros_like(dfy_train)\n",
    "\n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=model_pipeline, param_grid=param_grid, cv=10, scoring='neg_mean_squared_error', n_jobs=-1, error_score='raise')\n",
    "\n",
    "    # Perform KFold cross-validation -- score\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(dfX_train)):\n",
    "        X_train, X_test = dfX_train.iloc[train_index], dfX_train.iloc[test_index]\n",
    "        y_train, y_test = dfy_train.iloc[train_index], dfy_train.iloc[test_index]\n",
    "\n",
    "        # Fit GridSearchCV\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Best parameters\n",
    "        print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "        # Best estimator\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Predict\n",
    "        y_train_pred = best_model.predict(X_train)\n",
    "        y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "        # Calculate metrics for in-sample\n",
    "        in_sample_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "        in_sample_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "        in_sample_r2 = r2_score(y_train, y_train_pred)\n",
    "        in_sample_metrics.append((in_sample_rmse, in_sample_mae))\n",
    "\n",
    "        # Calculate metrics for out-of-sample\n",
    "        out_of_sample_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "        out_of_sample_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "        out_of_sample_r2 = r2_score(y_test, y_test_pred)\n",
    "        out_of_sample_metrics.append((out_of_sample_rmse, out_of_sample_mae))\n",
    "\n",
    "        # Collect out-of-sample predictions\n",
    "        out_of_sample_predictions[test_index] += y_test_pred\n",
    "\n",
    "    # Calculate average metrics\n",
    "    avg_in_sample_metrics = np.mean(in_sample_metrics, axis=0)\n",
    "    avg_out_of_sample_metrics = np.mean(out_of_sample_metrics, axis=0)\n",
    "\n",
    "    # Print average metrics\n",
    "    print(\"Average In-Sample Metrics (RMSE, MAE):\", avg_in_sample_metrics)\n",
    "    print(\"Average Out-of-Sample Metrics (RMSE, MAE):\", avg_out_of_sample_metrics)\n",
    "\n",
    "    # Scatter plot of actuals vs. average out-of-sample predictions\n",
    "    plt.scatter(dfy_train, out_of_sample_predictions)\n",
    "    plt.xlabel(\"Actual values\")\n",
    "    plt.ylabel(\"Out-of-sample predictions\")\n",
    "    plt.title(\"Scatter plot of Actuals vs. Out-of-Sample Predictions\")\n",
    "    return plt, out_of_sample_predictions, best_model\n",
    "\n",
    "def plot_actual_vs_predicted(model, dfy_train, out_of_sample_predictions):\n",
    "    # Apply exponential transformation\n",
    "    y_pred = np.exp(out_of_sample_predictions)\n",
    "    y_train = np.exp(dfy_train)\n",
    "    \n",
    "    true_rmse = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "    true_mae = mean_absolute_error(y_train, y_pred)\n",
    "\n",
    "    print(\"True RMSE:\", true_rmse)\n",
    "    print(\"True MAE:\", true_mae)\n",
    "    \n",
    "    # Create a DataFrame for scatter plot\n",
    "    scatter_data = pd.DataFrame({\n",
    "        'actual ticket price': y_train.values,\n",
    "        'predicted ticket price': y_pred\n",
    "    }, index = list(dfy[dfy.index!='MIN'].index))\n",
    "    \n",
    "    scatter_data.index.name = 'Team'\n",
    "\n",
    "    new_data = {'actual ticket price': np.exp(dfy_test).values[0], 'predicted ticket price': np.exp(model.predict(dfX_test))[0]}\n",
    "    scatter_data.loc['MIN'] = new_data\n",
    "    \n",
    "    # Define indices for outliers and test set\n",
    "    outliers_indices = ['NYK', 'LAL', 'GSW']  # Example indices for outliers\n",
    "    test_set_index = 'MIN'  # Example index for the test set\n",
    "    \n",
    "    # Create a new column in scatter_data for colors\n",
    "    scatter_data['color'] = 'blue'\n",
    "    scatter_data.loc[outliers_indices, 'color'] = 'red'\n",
    "    scatter_data.loc[test_set_index, 'color'] = 'green'\n",
    "    \n",
    "    # Extract x, y, and labels\n",
    "    x = scatter_data['actual ticket price']\n",
    "    y = scatter_data['predicted ticket price']\n",
    "    labels = scatter_data.index\n",
    "    colors = scatter_data['color']\n",
    "    \n",
    "    # Create the scatter plot with seaborn\n",
    "    fig, ax = plt.subplots(1, figsize=(15, 8))\n",
    "    fig.suptitle('Actual v Predicted Ticket Price')\n",
    "    \n",
    "    # Plot the scatter points with the respective colors\n",
    "    sns.scatterplot(x=x, y=y, hue=colors, palette={'blue': 'lightblue', 'red': 'red', 'green': 'green'}, ax=ax, s=50, edgecolor='k', alpha=0.7, legend=False)\n",
    "    \n",
    "    # Add a regression line\n",
    "    sns.regplot(x=\"actual ticket price\", y=\"predicted ticket price\", data=scatter_data, scatter=False, ax=ax, color='grey')\n",
    "    \n",
    "    # Annotate the points with team names\n",
    "    for i, txt in enumerate(labels):\n",
    "        ax.annotate(txt, (x[i], y[i]), xytext=(0, 5), textcoords='offset points')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    \n",
    "    return plt, scatter_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split\n",
    "\n",
    "Because we only have 30 obs, in order to test our model we will leave out only 1 record as a hold out set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX, dfy = df.drop(target,axis=1), np.log(df[target])\n",
    "print(dfX.shape)\n",
    "\n",
    "print('num_attribs: ',len(num_ls))\n",
    "print('cat_attribs: ',len(bool_ls))\n",
    "\n",
    "#======== train test split =========== manual split. 1 record holdout\n",
    "# dfX_train, dfX_test, dfy_train, dfy_test = train_test_split(dfX, dfy, test_size=0.2, random_state=123)\n",
    "dfX_train = dfX[dfX.index!= 'MIN']\n",
    "dfX_test = dfX[dfX.index == 'MIN']\n",
    "dfy_train = dfy[dfy.index!= 'MIN']\n",
    "dfy_test = dfy[dfy.index == 'MIN']\n",
    "\n",
    "dfX_train = dfX_train.reset_index()\n",
    "dfX_test = dfX_test.reset_index()\n",
    "dfy_train = dfy_train.reset_index()\n",
    "dfy_test = dfy_test.reset_index()\n",
    "\n",
    "#reorder cats to the back\n",
    "dfX_train = dfX_train[num_ls+bool_ls]\n",
    "dfX_test = dfX_test[num_ls+bool_ls]\n",
    "\n",
    "dfy_train = dfy_train[target]\n",
    "dfy_test = dfy_test[target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('_____________________________')\n",
    "print('train / test')\n",
    "print(dfX_train.shape,'/',dfX_test.shape)\n",
    "print(dfy_train.shape,'/',dfy_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the regressor\n",
    "The output of this cell will be the best alpha found for all CVs. Average in sample and out of sample metrics and the scatter plot for in sample and out of sample predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regressor: The name of the regression algorithm \n",
    "Regressor = Lasso\n",
    "# param_grid (dictionary): list out all the hyperparameters for the algorithm selected and the respective range for them\n",
    "param_grid = {'regressor__alpha': np.logspace(-4, 0, 50)}\n",
    "plt, out_of_sample_predictions, model = cross_val_tune_regressor(dfX_train, dfy_train, num_ls, bool_ls, Regressor, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot actual values of the target using scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt, scatter_data = plot_actual_vs_predicted(model, dfy_train, out_of_sample_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate prediction intervals\n",
    "to calculate prediction intervals fro each record with 95% confidence to get a better view of the redictions from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rep_cross_val_tune_regressor(dfX_train, dfy_train, num_ls, bool_ls, Regressor, param_grid):\n",
    "    \"\"\"\n",
    "    Perform repeated cross-validation for hyperparameter tuning and evaluation of a regression model to come up with prediction intervals\n",
    "    \n",
    "    Parameters:\n",
    "    - dfX_train (pd.DataFrame): Training features data.\n",
    "    - dfy_train (pd.Series or pd.DataFrame): Training target data.\n",
    "    - num_ls (list): List of indices or column names for numerical features.\n",
    "    - bool_ls (list): List of indices or column names for boolean features.\n",
    "    - Regressor (sklearn estimator): Regressor class to be used.\n",
    "    - param_grid (dict): Dictionary with parameters names and lists of parameter settings to try.\n",
    "    \n",
    "    Returns:\n",
    "    - out_of_sample_predictions_df (pd.DataFrame): DataFrame containing out-of-sample predictions for each fold.\n",
    "    - in_sample_metrics (list): List of tuples containing in-sample RMSE and MAE for each fold.\n",
    "    - out_of_sample_metrics (list): List of tuples containing out-of-sample RMSE and MAE for each fold.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define numerical and boolean pipelines\n",
    "    num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')), \n",
    "    ('robust', RobustScaler()),\n",
    "    ('yeojohnson', FunctionTransformer())  # Placeholder for ConditionalTransformer\n",
    "    ])\n",
    "\n",
    "    bool_pipeline = Pipeline([\n",
    "        ('identity', FunctionTransformer())\n",
    "    ])\n",
    "\n",
    "    # Combine pipelines with ColumnTransformer\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', num_pipeline, num_ls),\n",
    "        ('bool', bool_pipeline, bool_ls)\n",
    "    ])\n",
    "\n",
    "    # Combine preprocessor with model in a pipeline\n",
    "    model_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', Regressor())\n",
    "    ])\n",
    "\n",
    "    # Initialize KFold\n",
    "    rkf = RepeatedKFold(n_splits=10, n_repeats=10, random_state=0)\n",
    "\n",
    "    num_folds = 10 * 10  # 10 splits * 10 repeats\n",
    "    out_of_sample_predictions_df = pd.DataFrame(np.zeros((dfX_train.shape[0], num_folds)))\n",
    "\n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=model_pipeline, param_grid=param_grid, cv=10, scoring='neg_mean_squared_error', n_jobs=-1, error_score='raise')\n",
    "\n",
    "    # Perform KFold cross-validation -- score\n",
    "    for fold_idx, (train_index, test_index) in enumerate(rkf.split(dfX_train)):\n",
    "        X_train, X_test = dfX_train.iloc[train_index], dfX_train.iloc[test_index]\n",
    "        y_train, y_test = dfy_train.iloc[train_index], dfy_train.iloc[test_index]\n",
    "\n",
    "        # Fit GridSearchCV\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Best parameters\n",
    "        # print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "        # Best estimator\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Predict\n",
    "        y_train_pred = best_model.predict(X_train)\n",
    "        y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "        out_of_sample_predictions_df.iloc[test_index, fold_idx] = y_test_pred\n",
    "\n",
    "    out_of_sample_predictions_df.columns = [f'y_pred{i+1}' for i in range(num_folds)]\n",
    "\n",
    "    #get upper, lower and median from the above table for each record to create prediction intervals\n",
    "    out_of_sample_predictions_df['lower'] = out_of_sample_predictions_df[out_of_sample_predictions_df > 0].min(axis=1)\n",
    "    out_of_sample_predictions_df['upper'] = out_of_sample_predictions_df[out_of_sample_predictions_df > 0].max(axis=1)\n",
    "    out_of_sample_predictions_df['y_pred'] = out_of_sample_predictions_df[out_of_sample_predictions_df > 0].median(axis=1)\n",
    "    out_of_sample_predictions_df['y_true'] = dfy_train\n",
    "\n",
    "    out_of_sample_predictions_df['lower'] = np.exp(out_of_sample_predictions_df['lower'])\n",
    "    out_of_sample_predictions_df['upper'] = np.exp(out_of_sample_predictions_df['upper'])\n",
    "    out_of_sample_predictions_df['y_pred'] = np.exp(out_of_sample_predictions_df['y_pred'])\n",
    "    out_of_sample_predictions_df['y_true'] = np.exp(out_of_sample_predictions_df['y_true'])    \n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    \n",
    "    plt.errorbar(out_of_sample_predictions_df[\"y_true\"], out_of_sample_predictions_df[\"y_pred\"], \n",
    "    yerr=(out_of_sample_predictions_df[\"y_pred\"] - out_of_sample_predictions_df[\"lower\"], out_of_sample_predictions_df[\"upper\"] - out_of_sample_predictions_df[\"y_pred\"]),\n",
    "    ecolor='grey', linestyle='', marker = \"o\", capsize=5)\n",
    "    \n",
    "    plt.plot(plt.xlim(), plt.xlim(), color=\"lightgray\", scalex=False, scaley=False)\n",
    "    plt.xlabel('Actual Price ($)')\n",
    "    plt.ylabel('Predicted Price ($)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_cross_val_tune_regressor(dfX_train, dfy_train, num_ls, bool_ls, Regressor, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rep_cross_val_tune_regressor(dfX_train, dfy_train, dfX_test, dfy_test, num_ls, bool_ls, Regressor, param_grid):\n",
    "    \"\"\"\n",
    "    Perform repeated cross-validation for hyperparameter tuning and evaluation of a regression model to come up with prediction intervals,\n",
    "    and predict on a holdout sample.\n",
    "    \n",
    "    Parameters:\n",
    "    - dfX_train (pd.DataFrame): Training features data.\n",
    "    - dfy_train (pd.Series or pd.DataFrame): Training target data.\n",
    "    - dfX_test (pd.DataFrame): Test features data (holdout sample).\n",
    "    - dfy_test (pd.Series or pd.DataFrame): Test target data (holdout sample).\n",
    "    - num_ls (list): List of indices or column names for numerical features.\n",
    "    - bool_ls (list): List of indices or column names for boolean features.\n",
    "    - Regressor (sklearn estimator): Regressor class to be used.\n",
    "    - param_grid (dict): Dictionary with parameters names and lists of parameter settings to try.\n",
    "    \n",
    "    Returns:\n",
    "    - out_of_sample_predictions_df (pd.DataFrame): DataFrame containing out-of-sample predictions for each fold.\n",
    "    - in_sample_metrics (list): List of tuples containing in-sample RMSE and MAE for each fold.\n",
    "    - out_of_sample_metrics (list): List of tuples containing out-of-sample RMSE and MAE for each fold.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define numerical and boolean pipelines\n",
    "    num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')), \n",
    "    ('robust', RobustScaler()),\n",
    "    ('yeojohnson', FunctionTransformer())  # Placeholder for ConditionalTransformer\n",
    "    ])\n",
    "\n",
    "    bool_pipeline = Pipeline([\n",
    "        ('identity', FunctionTransformer())\n",
    "    ])\n",
    "\n",
    "    # Combine pipelines with ColumnTransformer\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', num_pipeline, num_ls),\n",
    "        ('bool', bool_pipeline, bool_ls)\n",
    "    ])\n",
    "\n",
    "    # Combine preprocessor with model in a pipeline\n",
    "    model_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', Regressor())\n",
    "    ])\n",
    "\n",
    "    # Initialize KFold\n",
    "    rkf = RepeatedKFold(n_splits=10, n_repeats=10, random_state=0)\n",
    "\n",
    "    num_folds = 10 * 10  # 10 splits * 10 repeats\n",
    "    out_of_sample_predictions_df = pd.DataFrame(np.zeros((dfX_train.shape[0], num_folds)))\n",
    "    holdout_predictions = []\n",
    "\n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=model_pipeline, param_grid=param_grid, cv=10, scoring='neg_mean_squared_error', n_jobs=-1, error_score='raise')\n",
    "\n",
    "    # Perform KFold cross-validation -- score\n",
    "    for fold_idx, (train_index, test_index) in enumerate(rkf.split(dfX_train)):\n",
    "        X_train, X_test = dfX_train.iloc[train_index], dfX_train.iloc[test_index]\n",
    "        y_train, y_test = dfy_train.iloc[train_index], dfy_train.iloc[test_index]\n",
    "\n",
    "        # Fit GridSearchCV\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Best parameters\n",
    "        # print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "        # Best estimator\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Predict\n",
    "        y_train_pred = best_model.predict(X_train)\n",
    "        y_test_pred = best_model.predict(X_test)\n",
    "        holdout_pred = best_model.predict(dfX_test)\n",
    "\n",
    "        out_of_sample_predictions_df.iloc[test_index, fold_idx] = y_test_pred\n",
    "        holdout_predictions.append(holdout_pred[0])\n",
    "\n",
    "    out_of_sample_predictions_df.columns = [f'y_pred{i+1}' for i in range(num_folds)]\n",
    "\n",
    "    # Get upper, lower and median from the above table for each record to create prediction intervals\n",
    "    out_of_sample_predictions_df['lower'] = out_of_sample_predictions_df[out_of_sample_predictions_df > 0].min(axis=1)\n",
    "    out_of_sample_predictions_df['upper'] = out_of_sample_predictions_df[out_of_sample_predictions_df > 0].max(axis=1)\n",
    "    out_of_sample_predictions_df['y_pred'] = out_of_sample_predictions_df[out_of_sample_predictions_df > 0].median(axis=1)\n",
    "    out_of_sample_predictions_df['y_true'] = dfy_train\n",
    "\n",
    "    out_of_sample_predictions_df['lower'] = np.exp(out_of_sample_predictions_df['lower'])\n",
    "    out_of_sample_predictions_df['upper'] = np.exp(out_of_sample_predictions_df['upper'])\n",
    "    out_of_sample_predictions_df['y_pred'] = np.exp(out_of_sample_predictions_df['y_pred'])\n",
    "    out_of_sample_predictions_df['y_true'] = np.exp(out_of_sample_predictions_df['y_true'])    \n",
    "\n",
    "    # Process holdout predictions\n",
    "    holdout_predictions_df = pd.DataFrame([holdout_predictions])\n",
    "    holdout_predictions_df['lower'] = holdout_predictions_df.min(axis=1)\n",
    "    holdout_predictions_df['upper'] = holdout_predictions_df.max(axis=1)\n",
    "    holdout_predictions_df['y_pred'] = holdout_predictions_df.median(axis=1)\n",
    "    holdout_predictions_df['y_true'] = dfy_test.values[0]\n",
    "\n",
    "    holdout_predictions_df['lower'] = np.exp(holdout_predictions_df['lower'])\n",
    "    holdout_predictions_df['upper'] = np.exp(holdout_predictions_df['upper'])\n",
    "    holdout_predictions_df['y_pred'] = np.exp(holdout_predictions_df['y_pred'])\n",
    "    holdout_predictions_df['y_true'] = np.exp(holdout_predictions_df['y_true'])\n",
    "\n",
    "    out_of_sample_predictions_df = pd.concat([out_of_sample_predictions_df, holdout_predictions_df])\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    \n",
    "    plt.errorbar(out_of_sample_predictions_df[\"y_true\"], out_of_sample_predictions_df[\"y_pred\"], \n",
    "    yerr=(out_of_sample_predictions_df[\"y_pred\"] - out_of_sample_predictions_df[\"lower\"], out_of_sample_predictions_df[\"upper\"] - out_of_sample_predictions_df[\"y_pred\"]),\n",
    "    ecolor='grey', linestyle='', marker=\"o\", capsize=5)\n",
    "    \n",
    "    # Highlight the holdout prediction\n",
    "    plt.scatter(holdout_predictions_df['y_true'], holdout_predictions_df['y_pred'], color='red', label='Holdout Sample', zorder=5)\n",
    "    \n",
    "    plt.plot(plt.xlim(), plt.xlim(), color=\"lightgray\", scalex=False, scaley=False)\n",
    "    plt.xlabel('Actual Price ($)')\n",
    "    plt.ylabel('Predicted Price ($)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    return holdout_predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_predictions_df = rep_cross_val_tune_regressor(dfX_train, dfy_train, dfX_test, dfy_test, num_ls, bool_ls, Regressor, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the red dot represents the holdout sample \"MIN\" and the correspondng prediction intervals for the same. For values you can dive into the dataframe below\n",
    "holdout_predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
